{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a97380b-1303-42ae-8a92-280d958d69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee54cec-c59f-48fc-873b-89df3c2a1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = '/Users/ceanders/.google/credentials/projects/redskins-rule/redskins-rule-docker-airflow-credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feddf9e8-b12a-41be-9a93-6740bb3842df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 11:34:21 WARN Utils: Your hostname, Chases-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.9 instead (on interface en0)\n",
      "24/03/12 11:34:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/12 11:34:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"/usr/local/Cellar/apache-spark/3.5.0/libexec/jars/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('test') \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adea047-30bf-4257-b839-32fba7ccdfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2000.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data complete...here's the row count 32\n",
      "processsed count...32\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2001.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data complete...here's the row count 32\n",
      "processsed count...64\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2002.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data complete...here's the row count 32\n",
      "processsed count...96\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2003.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...128\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2004.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...160\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2005.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...192\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2006.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...224\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2007.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...256\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2008.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...288\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2009.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data complete...here's the row count 32\n",
      "processsed count...320\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2010.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...352\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2011.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...384\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2012.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...416\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2013.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...448\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2014.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...480\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2015.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...512\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2016.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...544\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2017.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed count...576\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2018.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed count...608\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2019.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...640\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2020.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 32\n",
      "processsed count...672\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2021.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data complete...here's the row count 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed count...706\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2022.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed count...740\n",
      "grabbing file...gs://redskins-rule-nfl-game-data/raw/schedule/nfl_season_2023.parquet\n",
      "exploding data complete...here's the row count 1\n",
      "xform df row count\n",
      "transforming data complete...here's the row count 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1155:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed count...774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to read schema from a Parquet file and explode fields\n",
    "def get_raw_nfl_data_and_explode(file_path):\n",
    "    df = spark.read.parquet(file_path)\n",
    "    count = df.count()\n",
    "    print(f\"exploding data complete...here's the row count {count}\")\n",
    "    exploded_df = df.withColumn('exp_events', F.explode('events'))\n",
    "    exploded_df = exploded_df.withColumn('exp_competitions', F.explode('exp_events.competitions'))\n",
    "    exploded_df = exploded_df.withColumn('exp_competitors', F.explode('exp_competitions.competitors'))\n",
    "    return exploded_df\n",
    "\n",
    "# Define empty df to save to\n",
    "empty_RDD = spark.sparkContext.emptyRDD()\n",
    "nfl_columns = StructType([\n",
    "    StructField(\"date\", TimestampNTZType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "nfl_df = spark.createDataFrame(data = empty_RDD, schema = nfl_columns)\n",
    "\n",
    "# Get files from GCS and process\n",
    "def list_blobs_with_prefix(bucket_name, prefix):\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    return [\"gs://\" + bucket_name + \"/\" + blob.name for blob in blobs]\n",
    "\n",
    "nfl_bucket_name = 'redskins-rule-nfl-game-data'\n",
    "nfl_prefix = 'raw/schedule/'\n",
    "nfl_file_paths = list_blobs_with_prefix(nfl_bucket_name, nfl_prefix)\n",
    "\n",
    "for file_path in nfl_file_paths:\n",
    "    print(f\"grabbing file...{file_path}\")\n",
    "    exploded_df = get_raw_nfl_data_and_explode(file_path)\n",
    "    \n",
    "    # create a temp table\n",
    "    exploded_df.createOrReplaceTempView('temp')\n",
    "\n",
    "    # transform the file and save to a df\n",
    "    xform_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exp_events.date,\n",
    "        exp_competitors.id,\n",
    "        exp_competitors.score.value\n",
    "    FROM\n",
    "        temp\n",
    "    GROUP BY \n",
    "        1,2,3\n",
    "    \"\"\")\n",
    "    print(\"xform df row count\")\n",
    "    count = xform_df.count()\n",
    "    print(f\"transforming data complete...here's the row count {count}\") \n",
    "    \n",
    "    # union to the processed df\n",
    "    nfl_df = nfl_df.unionByName(xform_df)\n",
    "    proc_count = nfl_df.count()\n",
    "    print(f\"processsed count...{proc_count}\")\n",
    "spark.catalog.dropTempView('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35770e47-c35d-4f5d-88be-4159f1c9c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-----+---------------+------------------+---------------+\n",
      "|               date| id|value|winning_team_id|winning_team_score|redskins_result|\n",
      "+-------------------+---+-----+---------------+------------------+---------------+\n",
      "|2000-09-03 17:00:00| 28| 20.0|             28|              20.0|            WIN|\n",
      "|2000-09-10 20:15:00| 28| 10.0|              8|              15.0|           LOSE|\n",
      "|2000-09-18 16:00:00| 28|  0.0|             28|               0.0|            WIN|\n",
      "|2000-09-25 00:20:00| 28| 16.0|             28|              16.0|            WIN|\n",
      "|2000-10-01 20:15:00| 28| 20.0|             28|              20.0|            WIN|\n",
      "|2000-10-08 17:00:00| 28| 17.0|             28|              17.0|            WIN|\n",
      "|2000-10-15 17:00:00| 28| 10.0|             28|              10.0|            WIN|\n",
      "|2000-10-22 20:15:00| 28| 35.0|             28|              35.0|            WIN|\n",
      "|2000-10-30 17:00:00| 28|  0.0|             28|               0.0|            WIN|\n",
      "|2000-11-05 21:05:00| 28| 15.0|             22|              16.0|           LOSE|\n",
      "|2000-11-20 17:00:00| 28|  0.0|             28|               0.0|            WIN|\n",
      "|2000-11-26 18:00:00| 28| 20.0|             21|              23.0|           LOSE|\n",
      "|2000-12-03 18:00:00| 28|  7.0|             19|               9.0|           LOSE|\n",
      "|2000-12-10 21:15:00| 28| 13.0|              6|              32.0|           LOSE|\n",
      "|2000-12-16 17:30:00| 28|  3.0|             23|              24.0|           LOSE|\n",
      "|2000-12-24 18:00:00| 28| 20.0|             28|              20.0|            WIN|\n",
      "|2001-09-09 20:15:00| 28|  3.0|             24|              30.0|           LOSE|\n",
      "|2001-09-25 01:00:00| 28|  0.0|              9|               0.0|           LOSE|\n",
      "|2001-09-30 17:00:00| 28| 13.0|             12|              45.0|           LOSE|\n",
      "|2001-10-07 17:00:00| 28|  9.0|             19|              23.0|           LOSE|\n",
      "+-------------------+---+-----+---------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate win metrics and update df\n",
    "nfl_df.createOrReplaceTempView('nfl_df')\n",
    "nfl_win_metrics_query = \"\"\"\n",
    "SELECT\n",
    "  *,\n",
    "  CASE\n",
    "    WHEN id = winning_team_id THEN 'WIN'\n",
    "    ELSE 'LOSE'\n",
    "  END as redskins_result\n",
    "FROM (\n",
    "  SELECT \n",
    "    *,\n",
    "    MAX_BY(id, value) OVER(PARTITION BY date) as winning_team_id,\n",
    "    MAX_BY(value, value) OVER(PARTITION BY date) as winning_team_score\n",
    "  FROM \n",
    "    nfl_df\n",
    ")\n",
    "WHERE\n",
    "  id = '28'\n",
    "\"\"\"\n",
    "nfl_df = spark.sql(nfl_win_metrics_query)\n",
    "nfl_df.createOrReplaceTempView('nfl_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f1a9b2-b259-43f0-8b9c-77caaaca38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_elections_data(file_path):\n",
    "    df = spark.read.parquet(file_path)\n",
    "    return df\n",
    "\n",
    "elec_bucket_name = 'redskins-rule-presidential-election-data'\n",
    "elec_prefix = 'raw/'\n",
    "elec_file_paths = list_blobs_with_prefix(elec_bucket_name, elec_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b036e4-b274-4e48-b550-3d3785a435a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_columns = StructType([\n",
    "    StructField(\"row_num\", LongType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"candidate\", StringType(), True),\n",
    "    StructField(\"political_party\", StringType(), True),\n",
    "    StructField(\"electoral_votes\", LongType(), True),\n",
    "    StructField(\"popular_votes\", StringType(), True),\n",
    "    StructField(\"popular_percentage\", StringType(), True),\n",
    "])\n",
    "elec_df = spark.createDataFrame(data = empty_RDD, schema = elec_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeee8c3c-2258-46f7-961c-6b09df99306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbing file...gs://redskins-rule-presidential-election-data/raw/processed_elections.parquet\n"
     ]
    }
   ],
   "source": [
    "for file_path in elec_file_paths:\n",
    "    print(f\"grabbing file...{file_path}\")\n",
    "    df = get_raw_elections_data(file_path)\n",
    "    elec_df = elec_df.unionAll(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faeee5c1-d179-4546-9486-ad6aca7d9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format year field\n",
    "elec_df = elec_df.withColumn('year', F.to_date(elec_df.year, 'yyyy'))\n",
    "elec_df = elec_df.withColumn('popular_votes', F.translate(elec_df.popular_votes, \",\", \"\").cast(LongType()))\n",
    "# filter for years > 1996 (need pre-2000 data for incumbent status)\n",
    "query = \"year >= DATE '1996-01-01'\"\n",
    "elec_df = elec_df.where(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef84331-44c0-449a-b7fd-b932eeab661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add election dates to elections df:\n",
    "# Get date of election by election year\n",
    "def find_election_day(year):\n",
    "    # Find the first day of November\n",
    "    date = datetime(year, 11, 1)\n",
    "    # If this day is not Monday, find the next Monday\n",
    "    while date.weekday() != 0:\n",
    "        date += timedelta(days=1)\n",
    "    # The election day is the next Tuesday\n",
    "    date += timedelta(days=1)\n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Find the election days every 4 years starting in the year 1996\n",
    "elec_dates_values = [find_election_day(year) for year in range(1996, datetime.now().year + 1, 4)]\n",
    "# convert to pandas df to load to spark\n",
    "pd_df = pd.DataFrame(elec_dates_values, columns=['elec_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807a6cc4-fd07-49fd-94ee-a57d925f2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the spark df to attach to elec_df\n",
    "elec_dates_columns = StructType([\n",
    "    StructField(\"elec_date\", StringType(), True),\n",
    "])\n",
    "elec_dates_df = spark.createDataFrame(data = pd_df, schema = elec_dates_columns)\n",
    "elec_dates_df = elec_dates_df.withColumn('elec_date', F.to_date(elec_dates_df.elec_date, 'yyyy-MM-dd'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "135159df-325d-45ca-b52e-82049a4a623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join em\n",
    "elec_df.createOrReplaceTempView('elec_df')\n",
    "elec_dates_df.createOrReplaceTempView('elec_dates_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a1b89d-cf09-4112-8028-86ec2744ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_join_query = \"\"\"\n",
    "    SELECT e.*, ed.elec_date\n",
    "    FROM elec_df e \n",
    "    LEFT JOIN (SELECT DISTINCT elec_date FROM elec_dates_df) ed ON DATE_TRUNC('year', e.year) = DATE_TRUNC('year', ed.elec_date)\n",
    "\"\"\"\n",
    "elec_df = spark.sql(dates_join_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d157e2-4f6a-4796-abf8-6a9666d6254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate winning party metrics\n",
    "elec_df.createOrReplaceTempView('elec_df')\n",
    "elec_win_metrics_query = \"\"\"\n",
    "SELECT\n",
    "  *,\n",
    "  CASE WHEN (pres_winning_party = pop_incumbent_party) THEN 'WIN'\n",
    "  ELSE 'LOSE'\n",
    "  END as pop_incumbent_elec_result\n",
    "FROM (\n",
    "  SELECT\n",
    "    *,\n",
    "    LAG(pres_winning_party, 1) OVER (ORDER BY elec_date) as incumbent_pres_party,\n",
    "    LAG(pop_winning_party, 1) OVER (ORDER BY elec_date) as pop_incumbent_party\n",
    "  FROM (\n",
    "    SELECT\n",
    "      elec_date,\n",
    "      MAX_BY(political_party, electoral_votes) as pres_winning_party,\n",
    "      MAX_BY(candidate, electoral_votes) as pres_winning_candidate,\n",
    "      MAX_BY(electoral_votes, electoral_votes) as count_electoral_votes,\n",
    "      MAX_BY(popular_votes, popular_votes) as count_popular_votes,\n",
    "      pop_winning_candidate,\n",
    "      pop_winning_party,\n",
    "      electoral_rank_desc,\n",
    "      popular_rank_desc,\n",
    "      challenger_pres_party\n",
    "    FROM (\n",
    "      SELECT \n",
    "        foo.*,\n",
    "        bar.challenger_pres_party,\n",
    "        RANK() OVER (PARTITION BY foo.elec_date ORDER BY electoral_votes DESC) as electoral_rank_desc,\n",
    "        RANK() OVER (PARTITION BY foo.elec_date ORDER BY popular_votes DESC) as popular_rank_desc\n",
    "      FROM (\n",
    "        SELECT\n",
    "          elec_date,\n",
    "          candidate,\n",
    "          political_party,\n",
    "          electoral_votes,\n",
    "          popular_votes,\n",
    "          MAX_BY(candidate, popular_votes) OVER (PARTITION BY elec_date) as pop_winning_candidate,\n",
    "          MAX_BY(political_party, popular_votes) OVER (PARTITION BY elec_date) as pop_winning_party\n",
    "        FROM\n",
    "          elec_df\n",
    "      ) foo\n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          elec_date,\n",
    "          political_party as challenger_pres_party\n",
    "        FROM (\n",
    "          SELECT \n",
    "            *,\n",
    "            RANK() OVER (PARTITION BY elec_date ORDER BY electoral_votes DESC) as electoral_rank_desc\n",
    "          FROM (\n",
    "            SELECT\n",
    "              *,\n",
    "              LAG(pres_winning_party, 1) OVER (ORDER BY elec_date) as prev_winning_party\n",
    "            FROM (\n",
    "              SELECT\n",
    "              elec_date,\n",
    "              political_party,\n",
    "              electoral_votes,\n",
    "              MAX_BY(political_party, electoral_votes) OVER(PARTITION BY elec_date) as pres_winning_party\n",
    "            FROM \n",
    "              elec_df\n",
    "            )\n",
    "          )\n",
    "          WHERE \n",
    "            political_party <> prev_winning_party\n",
    "        )\n",
    "        WHERE \n",
    "          electoral_rank_desc = 1\n",
    "      ) bar\n",
    "      ON foo.elec_date = bar.elec_date\n",
    "    )\n",
    "    WHERE \n",
    "      electoral_rank_desc = 1\n",
    "    GROUP BY \n",
    "      elec_date,\n",
    "      electoral_rank_desc,\n",
    "      popular_rank_desc,\n",
    "      pop_winning_candidate,\n",
    "      pop_winning_party,\n",
    "      challenger_pres_party\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "elec_df = spark.sql(elec_win_metrics_query)\n",
    "elec_df.createOrReplaceTempView('elec_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d3e85b-f080-45a9-87df-e365ba44da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join nfl to elections\n",
    "elec_nfl_join_query = \"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT \n",
    "    *,\n",
    "    RANK() OVER(PARTITION BY elec_date ORDER BY date_diff ASC) diff_rank_asc\n",
    "  FROM (\n",
    "    SELECT *,\n",
    "    DATEDIFF(day, n.date, e.elec_date) date_diff\n",
    "    FROM\n",
    "      elec_df e\n",
    "    LEFT JOIN \n",
    "      nfl_df n\n",
    "    ON (DATEDIFF(day, n.date, e.elec_date) BETWEEN 0 AND 10)\n",
    "  )\n",
    ")\n",
    "WHERE diff_rank_asc = 1\n",
    "\"\"\"\n",
    "nfl_elec_df = spark.sql(elec_nfl_join_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d88399-1def-4346-99a7-f885e3b24e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_prediction_values(df_elems)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction functions\n",
    "def predict_pres(df_elem):\n",
    "    if df_elem.redskins_result == 'WIN':\n",
    "        return [df_elem.elec_date, df_elem.incumbent_pres_party]\n",
    "    else: \n",
    "        return [df_elem.elec_date, df_elem.challenger_pres_party]\n",
    "\n",
    "def predict_pres_flipped(df_elem):\n",
    "    if df_elem.redskins_result == 'LOSE':\n",
    "        return [df_elem.elec_date, df_elem.incumbent_pres_party]\n",
    "    else: \n",
    "        return [df_elem.elec_date, df_elem.challenger_pres_party]\n",
    "        \n",
    "def determine_rule(df_elem):\n",
    "    if df_elem.pop_winning_party != df_elem.pres_winning_party:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_prediction_values(df_elems):\n",
    "    prediction_values = []\n",
    "    rule_toggle = determine_rule(df_elems[0])\n",
    "    for elem in df_elems:\n",
    "        # check toggle for which function to run \n",
    "        if rule_toggle == 1:\n",
    "            prediction_values.append(predict_pres(elem))\n",
    "        elif rule_toggle == -1:\n",
    "            prediction_values.append(predict_pres_flipped(elem))\n",
    "        else: \n",
    "            print('TOGGLE ERROR')\n",
    "        # determine toggle for next iteration\n",
    "        rule_toggle = determine_rule(elem)\n",
    "    return prediction_values\n",
    "\n",
    "spark.udf.register('predict_pres', predict_pres)\n",
    "spark.udf.register('predict_pres_flipped', predict_pres_flipped)\n",
    "spark.udf.register('determine_rule', determine_rule)\n",
    "spark.udf.register('get_prediction_values', get_prediction_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2fcd98e-028a-44ae-8fa7-baecbbb32a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 11:36:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:36:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# get df elems to iterate through for prediction functions\n",
    "nfl_elec_df_elems = nfl_elec_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e3e51c-d62c-45bb-b692-82cc0ec5e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_values = get_prediction_values(nfl_elec_df_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed6455b-cdd0-41ab-8907-7b886a463140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of prediction values\n",
    "pd_df = pd.DataFrame(prediction_values, columns=['p_elec_date', 'prediction'])\n",
    "predictions_df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d007bbb0-5153-492a-80c4-0aa92a91049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_elec_df = nfl_elec_df.join(predictions_df, predictions_df.p_elec_date == nfl_elec_df.elec_date, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543a0248-68e2-49f7-aed6-61e727979ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.check_rule(pres_winning_party, prediction)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check prediction against elec result\n",
    "def check_rule(pres_winning_party, prediction):\n",
    "    return pres_winning_party == prediction\n",
    "spark.udf.register('check_rule', check_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb0d533-b380-4583-acde-fd940c758ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 11:39:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/03/12 11:39:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "nfl_elec_df = nfl_elec_df.withColumn('prediction_results', check_rule(nfl_elec_df.pres_winning_party, nfl_elec_df.prediction))\n",
    "nfl_elec_df = nfl_elec_df.where(nfl_elec_df.elec_date >= '2000-01-01')\n",
    "nfl_elec_df.toPandas().to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
